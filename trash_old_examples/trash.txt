# the whole trick is here.
def _select_config(
    self,
    Xraw: np.array,
    yraw: np.array,
    current: list,
    newpoint: np.array,

) -> np.ndarray:
    """Selects the next hyperparameter config to try.

    This function takes the formatted data, fits the GP model and optimizes the
    UCB acquisition function to select the next point.

    Args:
        Xraw: The un-normalized array of hyperparams, Time and
            Reward - reward is for example the reward of an rl agent
        yraw: The un-normalized vector of reward changes.
        current: The hyperparams of trials currently running. This is
            important so we do not select the same config twice. If there is
            data here then we fit a second GP including it
            (with fake y labels). The GP variance doesn't depend on the y
            labels so it is ok.
        newpoint: The Reward and Time for the new point.
            We cannot change these as they are based on the *new weights*.
        bounds: Bounds for the hyperparameters. Used to normalize.
        num_f: The number of fixed params. Almost always 2 (reward+time)

    Return:
        xt: A vector of new hyperparameters.
    """
    # for DKL

    neural_network = self.neural_network
    seed = self.seed 

    bounds = self._hyperparam_bounds_flat
    
    # append to Xraw similarity column
    Xraw_ = np.hstack([Xraw, np.full((Xraw.shape[0], 1), self.sim_feature)])
    # add meta data to our current trail data
    X_train = np.concatenate([self.metadata_train_x.values, Xraw_], axis=1)
    # scale for neural net
    X_train_scaled = scaler.fit_transform(X_train)
    # important to standardise rewards, each in its own env - the meta is already standardised
    y = standardize(yraw).reshape(yraw.size, 1)
    y_train = np.concatenate([self.metadata_train_y.values, y], axis=1)
    
    # fixed = normalize(newpoint, oldpoints)
    logger.info('about to go train')


    likelihood = gpytorch.likelihoods.GaussianLikelihood()

    train_x = torch.tensor(X_train_scaled, dtype=torch.float32)
    train_y =torch.squeeze(torch.tensor(y_train, dtype=torch.float32))
    
    m = self.deep_kernel_model(train_x, train_y, feature_extractor=neural_network, likelihood=likelihood,seed=seed)
    m_trained, mll_m ,l= metatrain_DKL_wilson(model=m, X_train=train_x, y_train=train_y, likelihood=likelihood,seed=seed)
    

    # if there are current runs you must freeze the neural network heart in order 
    # not to corrupt it with the fake values
    if current is None:
        m1_trained = deepcopy(m)
        l1 = deepcopy(l)
    else:
        # add the current trials to the dataset
        logger.info('training new gp')
        newpoint= newpoint.astype(np.float32)
        padding = np.array([newpoint for _ in range(current.shape[0])])
        current = np.hstack((padding, newpoint))
        current_ =np.hstack([current, np.full((current.shape[0], 1), self.sim_feature)])
        Xnew = np.vstack((X_train, current_))
        # fake labels bc we dont depend on y for variance calculation
        ypad = np.zeros(current.shape[0])
        ypad = ypad.reshape(-1, 1)
        ynew = np.vstack((y_train, ypad))
        scaler_new = MinMaxScaler(feature_range=(0, 1))
        Xnew_scaled = scaler_new.fit_transform(Xnew)
        train_xnew = torch.tensor(Xnew_scaled, dtype=torch.float32)
        train_ynew = torch.squeeze(torch.tensor(ynew, dtype=torch.float32))
        likelihood = gpytorch.likelihoods.GaussianLikelihood()
        m1 = self.deep_kernel_model(train_xnew, train_ynew, feature_extractor=neural_network, likelihood=likelihood,seed=seed)
        m1_trained, mll_m1,l1 = metatrain_DKL_wilson(model=m1, X_train=train_xnew, y_train=train_ynew, likelihood=likelihood, freeze=True,seed=seed)

    #xt = minimise_wrt_acq()
    xt = optimize_acq_DKL(UCB_DKL, m_trained, m1_trained,l,l1, fixed, num_f,seed)

    # convert back...denormalise
    xt = xt * (np.max(base_vals, axis=0) - np.min(base_vals, axis=0)) + np.min(
        base_vals, axis=0
    )

    xt = xt.astype(np.float32)
    return xt




        #print(pd.DataFrame(data=y).describe())
    #exit(0)
    # train_x,train_y, x_test, y_test = generate_dummy_data()

    # train_x= np.array(train_x)
    # scaler = StandardScaler()
    # train_x  = scaler.fit_transform(train_x)
    # old_lims = np.concatenate(
    # (np.max(train_x, axis=0), np.min(train_x, axis=0))
    #     ).reshape(2, train_x.shape[1])
    # #train_x = normalize(train_x ,old_lims )
    # train_x = torch.tensor(train_x)
    
    # x_test= np.array(x_test)
    # old_lims_test = np.concatenate(
    # (np.max(x_test, axis=0), np.min(x_test, axis=0))
    #     ).reshape(2, x_test.shape[1])
    # #x_test = normalize(x_test ,old_lims_test )
    # x_test = scaler.transform(x_test)
    # x_test = torch.tensor(x_test)




    X,y, x_test, y_test = generate_dummy_data()
    # y = np.array(y).reshape(-1, 1) 
    # y_test = np.array(y_test).reshape(-1,1 )


    # X= np.array(X)


    # # bounds = {'hp': [-5,5]}
    # # length = select_length(X, y, bounds, 0)

    # # X = X[-length:, :]
    # # y = y[-length:]

    # old_lims = np.concatenate(
    # (np.max(X, axis=0), np.min(X, axis=0))
    #     ).reshape(2, X.shape[1])


    # X = normalize(X ,old_lims )

    # x_test= np.array(x_test)
    # old_lims_test = np.concatenate(
    # (np.max(x_test, axis=0), np.min(x_test, axis=0))
    #     ).reshape(2, x_test.shape[1])
    # x_test = normalize(x_test ,old_lims_test )
    

# env_config =  {'env_config': {'gravity': 0.01}, 'env_name': 'CARLLunarLander'}
# env = CARLLunarLanderWrapper(contexts={0:{'GRAVITY_X':10}})
# env.reset(seed=0)
# print(env.step(0))

# # env_config =  {'LINK_LENGTH_1': 1}
# # env = CARLAcrobotWrapper(contexts={0:env_config})
# # env.reset(seed=0)
# # print(env.step(0))
# from ray.rllib.algorithms import ppo

# from ray.tune.registry import register_env
# def env_creator(env_config):
#     env_name = env_config['env_name']
#     env_context = env_config['env_config']
#     if env_name == 'CARLCartPole':
#         env = CARLCartPoleWrapper(contexts={0:env_context})
#     elif env_name == 'CARLMountainCar':
#         env = CARLMountainCarWrapper(contexts={0:env_context})
#     elif env_name == 'CARLMountainCarCont':
#         env=CARLMountainCarContWrapper(contexts={0:env_context})
#     elif env_name == 'CARLAcrobot':
#         env=CARLAcrobotWrapper(contexts={0:env_context})
#     elif env_name == 'CARLPendulum':
#         env=CARLPendulumWrapper(contexts={0:env_context})
#     else:
#         raise NotImplementedError

#     return env

# # env_name='CARLMountainCar'

# register_env(env_name, env_creator)
# algo = ppo.PPO(env="CARLMountainCar",config={
#     "env_config": env_config,  # config to pass to env class
# })
    # def step(self, action):
    #     obs_context, r, term, trun, info = self.env.step(action)
    #     obs = obs_context['obs']
        
    #     # Cap the observation within the limits of the observation space
    #     obs = np.clip(obs, self.observation_space.low, self.observation_space.high)
        
    #     self.state = obs
    #     return obs, r, term, trun, info

    # def reset(self, seed=None, options=None):
    #     obs_context, info = self.env.reset(seed=seed, options=options)
    #     obs = obs_context['obs']
        
    #     # Cap the reset observation as well
    #     obs = np.clip(obs, self.observation_space.low, self.observation_space.high)
        
    #     self.state = obs
    #     return obs, info

    
# def train_policy(env_config, n_steps=1_000_000, num_envs=8):

#     config = copy.deepcopy(env_config)
#     env_name = config.pop('env_name')
#     env_context = config
#     envs = SubprocVecEnv([make_env(env_config, i) for i in range(num_envs)])
#     model = PPO(MlpPolicy, envs, verbose=1)
#     mean_reward_before_train, std_reward_before_train  = evaluate_policy(model, envs, n_eval_episodes=100)
#     print(f"mean_reward:{mean_reward_before_train:.2f} +/- {std_reward_before_train:.2f}")
#     model.learn(total_timesteps=num_envs)
#     n_total_steps = n_steps*n_envs
#     model.save(f'/home/fr/fr_fr/fr_zs53/DKL/metaPBT-2.0/DKL/EPI/trained_policies/PPO_{env_name}_{n_steps}_{str(env_context)}')
#     mean_reward, std_reward = evaluate_policy(model, envs, n_eval_episodes=100)
#     print(f"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}")
#     return model

def generate_sampler(seed, dist_type, bounds_tuple, num_samples):
            # loguniform_dist = tune.loguniform(1e-5, 1e-3)
        # samples_lr = [loguniform_dist.sample() for _ in range(args.num_samples)]
        
        # sample_iter_lr = iter(samples_lr)
        # get_lr_sample = lambda: next(sample_iter_lr)
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)

        dist = dist_type(bounds_tuple[0], bounds_tuple[1])
        samples = [dist.sample() for _ in range(num_samples)]
        sample_iterator = iter(samples)
        get_sample_func = lambda: next(sample_iterator)
        return get_sample_func


        # meta_data_dir_list = ['/home/fr/fr_fr/fr_zs53/DKL/metaPBT-2.0/testing_dir/2024-09-12_19:58:08_PPO_length_0.05_pb2_Size4_CARLCartPole_timesteps_total/pb2_CARLCartPole_seed0_length_0.05',
        #                       '/pfs/work7/workspace/scratch/fr_zs53-dkl_exps/cluster_logs/pb2.gravity.c1/2024-09-15_18:44:38_PPO_gravity_0.9800000000000001_pb2_Size8_CARLCartPole_timesteps_total/pb2_CARLCartPole_seed0_gravity_0.9800000000000001']



                # # LR SAMPLER
        # loguniform_dist = tune.loguniform(1e-5, 1e-3)
        # samples_lr = [loguniform_dist.sample() for _ in range(args.num_samples)]
        
        # sample_iter_lr = iter(samples_lr)
        # get_lr_sample = lambda: next(sample_iter_lr)